{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Part_A_Akansh.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/akansh12/CS6910_Assignment/blob/main/Part_A_Akansh.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torchvision\n",
        "import os\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F"
      ],
      "metadata": {
        "id": "bpsqz3GDsGAc"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import copy\n",
        "from tqdm.notebook import tqdm"
      ],
      "metadata": {
        "id": "4LSedwiPtkjL"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "qits0LAU9als",
        "outputId": "212d9793-d784-4418-f9a4-1f6e901490af",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Qurstion-1"
      ],
      "metadata": {
        "id": "YjdzUTcJtlJY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### HELPER FUNCTIONS\n",
        "def findConv2dOutShape(H_in,W_in,conv,pool=2):\n",
        "  kernel_size=conv.kernel_size\n",
        "  stride=conv.stride\n",
        "  padding=conv.padding\n",
        "  dilation=conv.dilation\n",
        "\n",
        "  H_out=np.floor((H_in+2*padding[0]-dilation[0]*(kernel_size[0]-1)-1)/stride[0]+1)\n",
        "  W_out=np.floor((W_in+2*padding[1]-dilation[1]*(kernel_size[1]-1)-1)/stride[1]+1)\n",
        "  if pool:\n",
        "    H_out/=pool\n",
        "    W_out/=pool\n",
        "  return int(H_out),int(W_out)\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "  device = torch.device(\"cuda\")\n",
        "else:\n",
        "  device = \"cpu\""
      ],
      "metadata": {
        "id": "Z8y2uPrRyIZb"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class conv_net(nn.Module):\n",
        "  def __init__(self, params): #num_filters, filter_size, activation_functions,dense_neurons,num_outputs\n",
        "    super(conv_net,self).__init__()\n",
        "    c_in, h_in, w_in = params[\"input_shape\"]\n",
        "    num_filters = params[\"num_filters\"]\n",
        "    filter_size = params[\"filter_size\"]\n",
        "    self.act_function = params[\"activation_functions\"]\n",
        "    dense_neurons = params[\"dense_neurons\"]\n",
        "    num_output = params[\"num_output\"]\n",
        "    self.dropout = params[\"dropout\"]\n",
        "\n",
        "\n",
        "    ####Conv layers\n",
        "    self.conv1 = nn.Conv2d(c_in, num_filters[0], kernel_size=filter_size[0])\n",
        "    h,w = findConv2dOutShape(h_in, w_in, self.conv1)\n",
        "\n",
        "    self.conv2 = nn.Conv2d(num_filters[0], num_filters[1], kernel_size=filter_size[1])\n",
        "    h,w = findConv2dOutShape(h, w, self.conv2)\n",
        "\n",
        "    self.conv3 = nn.Conv2d(num_filters[1], num_filters[2], kernel_size=filter_size[2])\n",
        "    h,w = findConv2dOutShape(h, w, self.conv3)\n",
        "\n",
        "\n",
        "    self.conv4 = nn.Conv2d(num_filters[2], num_filters[3], kernel_size=filter_size[3])\n",
        "    h,w = findConv2dOutShape(h, w, self.conv4)\n",
        "\n",
        "\n",
        "    self.conv5 = nn.Conv2d(num_filters[3], num_filters[4], kernel_size=filter_size[4])\n",
        "    h,w = findConv2dOutShape(h, w, self.conv5)\n",
        "    #FC layers\n",
        "    self.num_flatten = h*w*num_filters[4]\n",
        "    self.fc1 = nn.Linear(self.num_flatten, dense_neurons)\n",
        "    self.fc2 = nn.Linear(dense_neurons, num_output)\n",
        "\n",
        "  def forward(self,x):\n",
        "    x = self.act_function[0](self.conv1(x))\n",
        "    x = F.max_pool2d(x, 2, 2)\n",
        "    x = self.act_function[1](self.conv2(x))\n",
        "    x = F.max_pool2d(x, 2, 2)\n",
        "    x = self.act_function[2](self.conv3(x))\n",
        "    x = F.max_pool2d(x, 2, 2)\n",
        "    x = self.act_function[3](self.conv4(x))\n",
        "    x = F.max_pool2d(x, 2, 2)\n",
        "    x = self.act_function[4](self.conv5(x))\n",
        "    x = F.max_pool2d(x, 2, 2)\n",
        "    #Flatening the layers\n",
        "    x = x.view(-1, self.num_flatten)\n",
        "    \n",
        "    x = self.act_function[5](self.fc1(x))\n",
        "    x = F.dropout(x, self.dropout, training= self.training)\n",
        "    x = self.fc2(x)\n",
        "    x = F.log_softmax(x, dim=1)\n",
        "    return x   \n"
      ],
      "metadata": {
        "id": "mMwQGRtrtjec"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "params = {\"input_shape\": (3,224,224),\n",
        "          \"num_filters\": [8,16,32,64,128],\n",
        "          \"filter_size\":[3,3,3,3,3],\n",
        "          \"activation_functions\": [nn.functional.relu]*6,\n",
        "          \"dense_neurons\": 128,\n",
        "          \"num_output\":10,\n",
        "          \"dropout\": 0.2          \n",
        "          }"
      ],
      "metadata": {
        "id": "9x2I21oZ2ulR"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = conv_net(params)\n",
        "model.to(device);"
      ],
      "metadata": {
        "id": "HVPLAUCSFt4n"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. What is the total number of computations done by your network? (assume m filters in each layer of size k×k and n neurons in the dense layer)\n",
        "\n",
        "- Ans: \n",
        "\n",
        "2. What is the total number of parameters in your network? (assume m filters in each layer of size k×k and n neurons in the dense layer)\n",
        "\n",
        "- Ans: "
      ],
      "metadata": {
        "id": "XYiOmr9NGBGO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Question 2"
      ],
      "metadata": {
        "id": "7PKelzQjJxAw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import datasets\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "import os\n",
        "path2data_train=\"/content/drive/MyDrive/inaturalist_12K/train\"\n",
        "path2data_test = \"/content/drive/MyDrive/inaturalist_12K/val\""
      ],
      "metadata": {
        "id": "uqgWVmTXJu9f"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.transforms.transforms import RandomRotation\n",
        "# DATA-Augmentations\n",
        "train_transforms = transforms.Compose([\n",
        "transforms.Resize((224,224)),\n",
        "transforms.RandomRotation((-20,20)),\n",
        "transforms.RandomHorizontalFlip(p=0.5),\n",
        "transforms.RandomVerticalFlip(p=0.5),\n",
        "transforms.ToTensor(),\n",
        "transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n",
        "\n",
        "test_transforms = transforms.Compose([\n",
        "transforms.Resize((224,224)),\n",
        "transforms.ToTensor(),\n",
        "transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])"
      ],
      "metadata": {
        "id": "a4xMupVOJvL5"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = datasets.ImageFolder(path2data_train, train_transforms)\n",
        "test_data = datasets.ImageFolder(path2data_test, test_transforms)"
      ],
      "metadata": {
        "id": "QRNpSGS2NsaR"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(data.class_to_idx)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZEtK5bTBN7b5",
        "outputId": "977d7338-a389-4bc1-c2d6-834f8b031061"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'Amphibia': 0, 'Animalia': 1, 'Arachnida': 2, 'Aves': 3, 'Fungi': 4, 'Insecta': 5, 'Mammalia': 6, 'Mollusca': 7, 'Plantae': 8, 'Reptilia': 9}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### Spiliting the data into train-val\n",
        "n_val = int(np.floor(0.1 * len(data)))\n",
        "n_train = len(data) - n_val\n",
        "train_ds, val_ds = random_split(data, [n_train, n_val])"
      ],
      "metadata": {
        "id": "w6CbzaH-PRIi"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Number of datapoints in train: \", len(train_ds))\n",
        "print(\"Number of datapoints in val: \", len(val_ds))\n",
        "print(\"Number of datapoints in test: \", len(test_data))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tLHSpwEvPgSx",
        "outputId": "e51c0748-4e3a-483f-adcf-e69ac07a76bb"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of datapoints in train:  9000\n",
            "Number of datapoints in val:  999\n",
            "Number of datapoints in test:  2000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dl = DataLoader(train_ds, batch_size=32, shuffle=True)\n",
        "val_dl = DataLoader(val_ds, batch_size=64, shuffle=False)\n",
        "test_dl = DataLoader(test_data, batch_size=64, shuffle=False)"
      ],
      "metadata": {
        "id": "_Qdn1Ks2OulG"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "DD2gOOVSU0E1"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def accuracy(y_pred,y_true):\n",
        "    y_pred = torch.exp(y_pred)\n",
        "    top_p,top_class = y_pred.topk(1,dim = 1)\n",
        "    equals = top_class == y_true.view(*top_class.shape)\n",
        "    return torch.mean(equals.type(torch.FloatTensor))\n",
        "\n",
        "for param in model.parameters():\n",
        "    param.require_grad = True\n",
        "\n",
        "from torch import optim\n",
        "\n",
        "opt = optim.Adam(model.parameters(), lr=1e-3)\n",
        "model = model.to(device)\n",
        "loss_func = nn.NLLLoss()\n",
        "schedular = optim.lr_scheduler.ReduceLROnPlateau(opt,factor = 0.1,patience = 5)\n"
      ],
      "metadata": {
        "id": "cgi60DXx77CC"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def training(model, train_params):\n",
        "  epochs = train_params[\"num_epochs\"]\n",
        "  train_dl = train_params[\"train_dl\"]\n",
        "  loss_func = train_params[\"loss_func\"]\n",
        "  opt = train_params[\"opt\"]\n",
        "  val_dl = train_params[\"val_dl\"]\n",
        "\n",
        "  train_loss_hist = []\n",
        "  valid_loss_hist = []\n",
        "  train_acc_hist = []\n",
        "  valid_acc_hist = []\n",
        "  valid_loss_min = np.Inf\n",
        "\n",
        "  for i in range(epochs):\n",
        "      \n",
        "      train_loss = 0.0\n",
        "      valid_loss = 0.0\n",
        "      train_acc = 0.0\n",
        "      valid_acc = 0.0 \n",
        "      \n",
        "      \n",
        "      model.train()\n",
        "      \n",
        "      for images,labels in tqdm(train_dl):\n",
        "          \n",
        "          images = images.to(device)\n",
        "          labels = labels.to(device)\n",
        "          \n",
        "          ps = model(images)\n",
        "          loss = loss_func(ps,labels)\n",
        "          \n",
        "          opt.zero_grad()\n",
        "          loss.backward()\n",
        "          opt.step()\n",
        "          \n",
        "          train_acc += accuracy(ps,labels)\n",
        "          train_loss += loss.item()\n",
        "          \n",
        "      avg_train_acc = train_acc / len(train_dl)\n",
        "      train_acc_hist.append(avg_train_acc)\n",
        "      avg_train_loss = train_loss / len(train_dl)\n",
        "      train_loss_hist.append(avg_train_loss)\n",
        "          \n",
        "      model.eval()\n",
        "      with torch.no_grad():\n",
        "          \n",
        "          for images,labels in tqdm(val_dl):\n",
        "              \n",
        "              images = images.to(device)\n",
        "              labels = labels.to(device)\n",
        "              \n",
        "              ps = model(images)\n",
        "              loss = loss_func(ps,labels)\n",
        "              \n",
        "              valid_acc += accuracy(ps,labels)\n",
        "              valid_loss += loss.item()\n",
        "              \n",
        "              \n",
        "          avg_valid_acc = valid_acc / len(val_dl)\n",
        "          valid_acc_hist.append(avg_valid_acc)\n",
        "          avg_valid_loss = valid_loss / len(val_dl)\n",
        "          valid_loss_hist.append(avg_valid_loss)\n",
        "          \n",
        "          schedular.step(avg_valid_loss)\n",
        "          \n",
        "          if avg_valid_loss <= valid_loss_min:\n",
        "              print('Validation loss decreased ({:.6f} --> {:.6f}).   Saving model ...'.format(valid_loss_min,avg_valid_loss))\n",
        "              torch.save({\n",
        "                  'epoch' : i,\n",
        "                  'model_state_dict' : model.state_dict(),\n",
        "                  'optimizer_state_dict' : opt.state_dict(),\n",
        "                  'valid_loss_min' : avg_valid_loss\n",
        "              },'model.pth')\n",
        "              \n",
        "              valid_loss_min = avg_valid_loss\n",
        "              \n",
        "      print(\"Epoch : {} Train Loss : {:.6f} Train Acc : {:.6f}\".format(i+1,avg_train_loss,avg_train_acc))\n",
        "      print(\"Epoch : {} Valid Loss : {:.6f} Valid Acc : {:.6f}\".format(i+1,avg_valid_loss,avg_valid_acc))\n",
        "\n",
        "  return model.load_state_dict(torch.load(\"/content/model.pth\")['model_state_dict'])\n"
      ],
      "metadata": {
        "id": "0yIIjoRSPoDo"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_params = {\"num_epochs\": 30,\n",
        "                \"loss_func\": loss_func,\n",
        "                \"train_dl\":train_dl ,\n",
        "                \"val_dl\":val_dl, \n",
        "                \"test_dl\": test_dl,\n",
        "                \"path2weights\": \"./\",\n",
        "                \"opt\": opt\n",
        "\n",
        "               }"
      ],
      "metadata": {
        "id": "TK4rWRwFVqCL"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training(model, train_params)"
      ],
      "metadata": {
        "id": "Q-s4ak5_WrBu",
        "outputId": "c16ab596-764b-4405-9b89-ef4ac6e8aab4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 24%|██▍       | 69/282 [00:50<02:35,  1.37it/s]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_state_dict(torch.load(\"/content/model.pth\")['model_state_dict'])"
      ],
      "metadata": {
        "id": "FPuCYPpxHav8",
        "outputId": "25e30107-6db2-4983-df33-ef99c9519426",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "dX3prxeYHayp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "e9cbbm_DHzUw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "GwwXYiyuHzXD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "J4_rNgnnHzZV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "def preprocess_image(path2image):\n",
        "    image =Image.open(path2image)\n",
        "\n",
        "    preprocess = torchvision.transforms.Compose([\n",
        "    torchvision.transforms.Resize((224,224)),\n",
        "    torchvision.transforms.ToTensor(),\n",
        "    torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),])\n",
        "\n",
        "    image = preprocess(image)\n",
        "    image.unsqueeze_(0)\n",
        "\n",
        "    return image\n",
        "\n",
        "rnd_image = preprocess_image(\"/content/drive/MyDrive/inaturalist_12K/val/Animalia/016125d0354a1fc95ed911c85ddec844.jpg\")\n",
        "model = model.to(device)\n",
        "rnd_image = rnd_image.to(device)\n",
        "\n",
        "no_of_layers=0\n",
        "conv_layers=[]\n",
        "\n",
        "model_children=list(model.children())\n",
        "\n",
        "for child in model_children:\n",
        "  if type(child)==nn.Conv2d:\n",
        "    no_of_layers+=1\n",
        "    conv_layers.append(child)\n",
        "  elif type(child)==nn.Sequential:\n",
        "    for layer in child.children():\n",
        "      if type(layer)==nn.Conv2d:\n",
        "        no_of_layers+=1\n",
        "        conv_layers.append(layer)\n",
        "\n",
        "\n",
        "first_layer_output = conv_layers[0](rnd_image)\n",
        "n_row = 2\n",
        "n_col = 4\n",
        "\n",
        "fig, axs = plt.subplots(n_row,n_col, figsize = (10,10))\n",
        "f_count = 0\n",
        "for i in range(n_row):\n",
        "  for j in range(n_col):\n",
        "    axs[i,j].imshow(first_layer_output[0][f_count].cpu().detach().numpy(), cmap = \"gray\")\n",
        "    f_count += 1\n"
      ],
      "metadata": {
        "id": "qb6WqKdK-sMj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}